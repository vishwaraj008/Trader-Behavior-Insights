{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EXXdpz8z81Ie",
        "outputId": "34f1b7c6-f01f-4800-8922-ea5f7a1c40b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading CSVs...\n",
            "Loaded.\n",
            "Trader shape: (211224, 16)\n",
            "Trader cols: ['Account', 'Coin', 'Execution Price', 'Size Tokens', 'Size USD', 'Side', 'Timestamp IST', 'Start Position', 'Direction', 'Closed PnL', 'Transaction Hash', 'Order ID', 'Crossed', 'Fee', 'Trade ID', 'Timestamp']\n",
            "Sentiment shape: (2644, 4)\n",
            "Sentiment cols: ['timestamp', 'value', 'classification', 'date']\n",
            "Sentiment unique classes: ['Fear' 'Extreme Fear' 'Neutral' 'Greed' 'Extreme Greed']\n",
            "After merge, sentiment coverage: 0.9999715941370299\n",
            "Detected cols: coin account closed_pnl size_usd side\n",
            "✅ Saved CSV outputs in csv_files/:\n",
            "- csv_files/volume_by_sentiment.csv\n",
            "- csv_files/win_rate_by_sentiment.csv\n",
            "- csv_files/account_sentiment_profiles.csv\n",
            "- csv_files/top_account_stats.csv\n",
            "- csv_files/merged_trader_sentiment_processed.csv\n",
            "- csv_files/account_stats.csv\n",
            "- csv_files/risk_reward_by_sentiment.csv\n",
            "- csv_files/pnl_distribution_stats.csv\n",
            "✅ Done. Figures in outputs/, CSVs in csv_files/, report in ds_report.pdf\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# Extended Colab Notebook - Web3 Trading Team (Deep Insights)\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "from scipy.stats import skew, kurtosis\n",
        "\n",
        "# Style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams.update({\"figure.dpi\": 120})\n",
        "\n",
        "# Create required directories\n",
        "for d in [\"csv_files\", \"outputs\"]:\n",
        "    Path(d).mkdir(exist_ok=True)\n",
        "\n",
        "# ============================================================\n",
        "# 1. Load Data\n",
        "# ============================================================\n",
        "trader_data_url = \"https://drive.google.com/uc?id=1IAfLZwu6rJzyWKgBToqwSmmVYU6VbjVs\"\n",
        "sentiment_data_url = \"https://drive.google.com/uc?id=1PgQC0tO8XN-wqkNyghWc_-mnrYv_nhSf\"\n",
        "\n",
        "print(\"Loading CSVs...\")\n",
        "trader_df = pd.read_csv(trader_data_url, low_memory=False)\n",
        "sentiment_df = pd.read_csv(sentiment_data_url, low_memory=False)\n",
        "print(\"Loaded.\")\n",
        "\n",
        "print(\"Trader shape:\", trader_df.shape)\n",
        "print(\"Trader cols:\", trader_df.columns.tolist())\n",
        "print(\"Sentiment shape:\", sentiment_df.shape)\n",
        "print(\"Sentiment cols:\", sentiment_df.columns.tolist())\n",
        "\n",
        "# ============================================================\n",
        "# 2. Timestamp Parsing\n",
        "# ============================================================\n",
        "def parse_maybe_unix(series):\n",
        "    if np.issubdtype(series.dtype, np.number):\n",
        "        try:\n",
        "            return pd.to_datetime(series, unit=\"ms\", errors=\"coerce\")\n",
        "        except:\n",
        "            return pd.to_datetime(series, unit=\"s\", errors=\"coerce\")\n",
        "    return pd.to_datetime(series, errors=\"coerce\")\n",
        "\n",
        "# Trader timestamps\n",
        "trader_df[\"ts_numeric\"] = parse_maybe_unix(trader_df.get(\"Timestamp\"))\n",
        "trader_df[\"ts_ist\"] = pd.to_datetime(trader_df.get(\"Timestamp IST\"), dayfirst=True, errors=\"coerce\")\n",
        "trader_df[\"trade_time\"] = trader_df[\"ts_ist\"].combine_first(trader_df[\"ts_numeric\"])\n",
        "trader_df[\"trade_date\"] = trader_df[\"trade_time\"].dt.date\n",
        "\n",
        "# Sentiment timestamps\n",
        "sentiment_df[\"sent_date\"] = pd.to_datetime(sentiment_df[\"date\"], errors=\"coerce\").dt.date\n",
        "sentiment_df[\"sent_unix\"] = parse_maybe_unix(sentiment_df[\"timestamp\"])\n",
        "sentiment_df[\"sent_date_unix\"] = sentiment_df[\"sent_unix\"].dt.date\n",
        "sentiment_df[\"date_final\"] = sentiment_df[\"sent_date\"].fillna(sentiment_df[\"sent_date_unix\"])\n",
        "\n",
        "sentiment_df[\"classification\"] = sentiment_df[\"classification\"].astype(str).str.strip().str.title()\n",
        "sentiment_df[\"sent_value\"] = pd.to_numeric(sentiment_df[\"value\"], errors=\"coerce\")\n",
        "\n",
        "sentiment_clean = sentiment_df[[\"date_final\", \"classification\", \"sent_value\"]].dropna().rename(columns={\"date_final\":\"date\"})\n",
        "sentiment_clean[\"date\"] = pd.to_datetime(sentiment_clean[\"date\"]).dt.date\n",
        "\n",
        "print(\"Sentiment unique classes:\", sentiment_clean[\"classification\"].unique())\n",
        "\n",
        "# ============================================================\n",
        "# 3. Merge\n",
        "# ============================================================\n",
        "merged = trader_df.merge(sentiment_clean, left_on=\"trade_date\", right_on=\"date\", how=\"left\")\n",
        "\n",
        "# Fill missing sentiment by forward-fill\n",
        "daily_sent = sentiment_clean.set_index(\"date\").sort_index()\n",
        "all_dates = pd.date_range(trader_df[\"trade_date\"].min(), trader_df[\"trade_date\"].max())\n",
        "daily_sent_full = daily_sent.reindex(all_dates.date).ffill().reset_index().rename(columns={\"index\":\"date\"})\n",
        "merged = merged.merge(daily_sent_full, on=\"date\", how=\"left\", suffixes=(\"\",\"_ffill\"))\n",
        "merged[\"classification\"] = merged[\"classification\"].fillna(merged.get(\"classification_ffill\"))\n",
        "merged[\"sent_value\"] = merged[\"sent_value\"].fillna(merged.get(\"sent_value_ffill\"))\n",
        "merged.drop([c for c in merged.columns if c.endswith(\"_ffill\")], axis=1, inplace=True)\n",
        "\n",
        "print(\"After merge, sentiment coverage:\", merged[\"classification\"].notna().mean())\n",
        "\n",
        "# ============================================================\n",
        "# 4. Normalize & Detect Columns\n",
        "# ============================================================\n",
        "merged.columns = [c.lower().replace(\" \", \"_\") for c in merged.columns]\n",
        "\n",
        "def find_col(df, keywords):\n",
        "    for c in df.columns:\n",
        "        if all(k in c for k in keywords):\n",
        "            return c\n",
        "    return None\n",
        "\n",
        "col_coin = find_col(merged, [\"coin\"]) or \"coin\"\n",
        "col_account = find_col(merged, [\"account\"]) or \"account\"\n",
        "col_pnl = find_col(merged, [\"pnl\"]) or \"closed_pnl\"\n",
        "col_size_usd = find_col(merged, [\"size\",\"usd\"]) or \"size_usd\"\n",
        "col_side = find_col(merged, [\"side\"]) or \"side\"\n",
        "\n",
        "merged[\"closed_pnl\"] = pd.to_numeric(merged[col_pnl], errors=\"coerce\")\n",
        "merged[\"size_usd\"] = pd.to_numeric(merged[col_size_usd], errors=\"coerce\")\n",
        "merged[\"side_norm\"] = merged[col_side].astype(str).str.upper()\n",
        "merged[\"profitable\"] = merged[\"closed_pnl\"] > 0\n",
        "merged[\"hour\"] = merged[\"trade_time\"].dt.hour\n",
        "\n",
        "print(\"Detected cols:\", col_coin, col_account, col_pnl, col_size_usd, col_side)\n",
        "\n",
        "# ============================================================\n",
        "# 5. Analysis & Plots\n",
        "# ============================================================\n",
        "def save_fig(fig, name):\n",
        "    out = Path(\"outputs\")/name\n",
        "    fig.tight_layout()\n",
        "    fig.savefig(out, bbox_inches=\"tight\")\n",
        "    plt.close(fig)\n",
        "    return out\n",
        "\n",
        "fig_paths = []\n",
        "\n",
        "# 5.1 PnL distribution\n",
        "fig = plt.figure(figsize=(8,5))\n",
        "sns.boxplot(data=merged, x=\"classification\", y=\"closed_pnl\")\n",
        "plt.title(\"PnL Distribution by Sentiment\")\n",
        "fig_paths.append(save_fig(fig, \"pnl_distribution.png\"))\n",
        "\n",
        "# 5.2 Win rate\n",
        "win_rate = merged.groupby(\"classification\")[\"profitable\"].mean()*100\n",
        "fig = plt.figure(figsize=(6,4))\n",
        "sns.barplot(x=win_rate.index, y=win_rate.values)\n",
        "plt.title(\"Win Rate by Sentiment\")\n",
        "fig_paths.append(save_fig(fig, \"win_rate.png\"))\n",
        "\n",
        "# 5.3 Trade volume\n",
        "fig = plt.figure(figsize=(6,4))\n",
        "vol = merged.groupby(\"classification\")[\"size_usd\"].sum()\n",
        "sns.barplot(x=vol.index, y=vol.values)\n",
        "plt.title(\"Total Volume by Sentiment\")\n",
        "fig_paths.append(save_fig(fig, \"volume.png\"))\n",
        "\n",
        "# 5.4 Hourly volume\n",
        "fig = plt.figure(figsize=(10,5))\n",
        "hourly = merged.groupby([\"hour\",\"classification\"])[\"size_usd\"].sum().reset_index()\n",
        "sns.lineplot(data=hourly, x=\"hour\", y=\"size_usd\", hue=\"classification\")\n",
        "plt.title(\"Hourly Trade Volume by Sentiment\")\n",
        "fig_paths.append(save_fig(fig, \"hourly_volume.png\"))\n",
        "\n",
        "# 5.5 Volatility of PnL by sentiment\n",
        "pnl_vol = merged.groupby(\"classification\")[\"closed_pnl\"].std().sort_values(ascending=False)\n",
        "fig = plt.figure(figsize=(6,4))\n",
        "sns.barplot(x=pnl_vol.index, y=pnl_vol.values)\n",
        "plt.title(\"PnL Volatility by Sentiment (Std Dev)\")\n",
        "fig_paths.append(save_fig(fig, \"pnl_volatility.png\"))\n",
        "\n",
        "# 5.6 Next-day sentiment effect\n",
        "daily_pnl = merged.groupby(\"trade_date\")[\"closed_pnl\"].mean().reset_index()\n",
        "daily_pnl = daily_pnl.merge(sentiment_clean, left_on=\"trade_date\", right_on=\"date\", how=\"left\")\n",
        "daily_pnl[\"prev_sentiment\"] = daily_pnl[\"classification\"].shift(1)\n",
        "lag_summary = daily_pnl.groupby(\"prev_sentiment\")[\"closed_pnl\"].mean().dropna()\n",
        "fig = plt.figure(figsize=(6,4))\n",
        "sns.barplot(x=lag_summary.index, y=lag_summary.values)\n",
        "plt.title(\"Avg Daily PnL by Previous-Day Sentiment\")\n",
        "fig_paths.append(save_fig(fig, \"lagged_pnl.png\"))\n",
        "\n",
        "# 5.7 Average trade size by sentiment\n",
        "avg_size = merged.groupby(\"classification\")[\"size_usd\"].mean()\n",
        "fig = plt.figure(figsize=(6,4))\n",
        "sns.barplot(x=avg_size.index, y=avg_size.values)\n",
        "plt.title(\"Average Trade Size by Sentiment\")\n",
        "fig_paths.append(save_fig(fig, \"avg_trade_size.png\"))\n",
        "\n",
        "# 5.8 Skewness & kurtosis of PnL by sentiment\n",
        "pnl_stats = merged.groupby(\"classification\")[\"closed_pnl\"].agg([skew, kurtosis])\n",
        "pnl_stats.to_csv(\"csv_files/pnl_distribution_stats.csv\")\n",
        "\n",
        "# 5.9 Coin × Sentiment heatmap\n",
        "coin_sentiment_pnl = merged.groupby([col_coin,\"classification\"])[\"closed_pnl\"].mean().unstack()\n",
        "fig = plt.figure(figsize=(12,8))\n",
        "sns.heatmap(coin_sentiment_pnl.fillna(0), cmap=\"RdYlGn\", center=0, annot=False)\n",
        "plt.title(\"Coin vs Sentiment: Avg Closed PnL\")\n",
        "fig_paths.append(save_fig(fig, \"coin_sentiment_heatmap.png\"))\n",
        "\n",
        "# 5.10 Hourly PnL by sentiment\n",
        "hourly_pnl = merged.groupby([\"hour\",\"classification\"])[\"closed_pnl\"].mean().reset_index()\n",
        "fig = plt.figure(figsize=(10,5))\n",
        "sns.lineplot(data=hourly_pnl, x=\"hour\", y=\"closed_pnl\", hue=\"classification\")\n",
        "plt.title(\"Hourly Avg PnL by Sentiment\")\n",
        "fig_paths.append(save_fig(fig, \"hourly_pnl.png\"))\n",
        "\n",
        "# 5.11 Account sentiment profiles\n",
        "acct_sentiment = merged.groupby([col_account,\"classification\"])[\"closed_pnl\"].mean().unstack()\n",
        "acct_sentiment.to_csv(\"csv_files/account_sentiment_profiles.csv\")\n",
        "\n",
        "# 5.12 Accounts summary\n",
        "acct_stats = merged.groupby(col_account).agg(total_pnl=(\"closed_pnl\",\"sum\"),\n",
        "                                            avg_pnl=(\"closed_pnl\",\"mean\"),\n",
        "                                            trades=(\"closed_pnl\",\"count\"),\n",
        "                                            win_rate=(\"profitable\",\"mean\"))\n",
        "acct_stats.to_csv(\"csv_files/account_stats.csv\")\n",
        "\n",
        "# 5.13 Top accounts\n",
        "top_accts = acct_stats.sort_values(\"total_pnl\", ascending=False).head(10)\n",
        "fig = plt.figure(figsize=(8,5))\n",
        "sns.barplot(x=top_accts[\"total_pnl\"].values, y=top_accts.index)\n",
        "plt.title(\"Top 10 Accounts by PnL\")\n",
        "fig_paths.append(save_fig(fig, \"top_accounts.png\"))\n",
        "\n",
        "# ============================================================\n",
        "# 6. Export CSV Outputs\n",
        "# ============================================================\n",
        "merged.to_csv(\"csv_files/merged_trader_sentiment_processed.csv\", index=False)\n",
        "win_rate.round(2).to_csv(\"csv_files/win_rate_by_sentiment.csv\")\n",
        "vol.to_csv(\"csv_files/volume_by_sentiment.csv\")\n",
        "\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 7. Report\n",
        "# ============================================================\n",
        "with PdfPages(\"ds_report.pdf\") as pdf:\n",
        "    fig = plt.figure(figsize=(11,8.5))\n",
        "    plt.axis(\"off\")\n",
        "    plt.text(0.5,0.6,\"Web3 Trading Team - Data Science Assignment\",ha=\"center\",fontsize=18)\n",
        "    plt.text(0.5,0.45,\"Candidate: ds_<your_name>\",ha=\"center\",fontsize=14)\n",
        "    pdf.savefig(fig); plt.close(fig)\n",
        "\n",
        "    for fp in fig_paths:\n",
        "        img = plt.imread(fp)\n",
        "        fig = plt.figure(figsize=(11,8.5))\n",
        "        plt.imshow(img); plt.axis(\"off\")\n",
        "        pdf.savefig(fig); plt.close(fig)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ======================================================\n",
        "# K-Means Clustering – Add this after your merge & cleaning\n",
        "# ======================================================\n",
        "print(\"\\n=== Starting K-Means Clustering ===\")\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import silhouette_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# If you're adding this to your existing script, use 'merged' directly\n",
        "# If running separately, load the saved file:\n",
        "# df = pd.read_csv(\"csv_files/merged_trader_sentiment_processed.csv\", low_memory=False)\n",
        "# Then rename df to merged or use df below\n",
        "\n",
        "# Make a clean copy for ML\n",
        "df_ml = merged.copy()\n",
        "\n",
        "# Create a simple pnl_direction feature (-1 = loss, 0 = breakeven, 1 = profit)\n",
        "df_ml['pnl_direction'] = np.sign(df_ml['closed_pnl']).fillna(0)\n",
        "\n",
        "# Features we will use for clustering\n",
        "cluster_features = ['size_usd', 'sent_value', 'hour', 'pnl_direction']\n",
        "\n",
        "# Drop rows with missing values in these columns\n",
        "df_cluster = df_ml[cluster_features + ['classification', 'closed_pnl', 'trade_date']].dropna()\n",
        "\n",
        "if df_cluster.empty:\n",
        "    print(\"ERROR: No valid rows left after dropna(). Check for too many NaNs in size_usd, sent_value, hour, or closed_pnl.\")\n",
        "else:\n",
        "    print(f\"Clustering on {len(df_cluster)} valid trades.\")\n",
        "\n",
        "    # Scale the features (very important for K-Means)\n",
        "    scaler = StandardScaler()\n",
        "    scaled = scaler.fit_transform(df_cluster[cluster_features])\n",
        "\n",
        "    # Find best number of clusters (K)\n",
        "    inertias = []\n",
        "    sil_scores = []\n",
        "    K_range = range(2, 10)\n",
        "\n",
        "    for k in K_range:\n",
        "        km = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "        km.fit(scaled)\n",
        "        inertias.append(km.inertia_)\n",
        "        sil_scores.append(silhouette_score(scaled, km.labels_))\n",
        "\n",
        "    # Plot Elbow and Silhouette to help choose K\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
        "    ax[0].plot(K_range, inertias, 'bx-')\n",
        "    ax[0].set_title('Elbow Method (lower is better)')\n",
        "    ax[0].set_xlabel('Number of clusters (K)')\n",
        "    ax[0].set_ylabel('Inertia')\n",
        "\n",
        "    ax[1].plot(K_range, sil_scores, 'rx-')\n",
        "    ax[1].set_title('Silhouette Score (higher is better)')\n",
        "    ax[1].set_xlabel('Number of clusters (K)')\n",
        "    ax[1].set_ylabel('Silhouette Score')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"outputs/elbow_silhouette_kmeans.png\")\n",
        "    plt.show()\n",
        "\n",
        "    # Decide on K – look at the plots and choose (example: 5 or 6 is common)\n",
        "    optimal_k = 5   # ← CHANGE THIS based on your elbow/silhouette plots\n",
        "    print(f\"→ Using K = {optimal_k} clusters\")\n",
        "\n",
        "    # Run final K-Means\n",
        "    kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
        "    df_cluster['behavior_cluster'] = kmeans.fit_predict(scaled)\n",
        "\n",
        "    # Show summary of each cluster\n",
        "    cluster_summary = df_cluster.groupby('behavior_cluster').agg({\n",
        "        'size_usd': 'mean',\n",
        "        'sent_value': 'mean',\n",
        "        'closed_pnl': ['mean', 'count', 'std'],\n",
        "        'classification': lambda x: x.mode()[0] if not x.mode().empty else 'Unknown',\n",
        "        'pnl_direction': 'mean'\n",
        "    }).round(2)\n",
        "\n",
        "    print(\"\\nCluster Summary:\")\n",
        "    print(cluster_summary)\n",
        "\n",
        "    # Save the clustered data\n",
        "    df_cluster.to_csv(\"csv_files/trades_with_clusters.csv\", index=False)\n",
        "    print(\"→ Saved clustered trades to csv_files/trades_with_clusters.csv\")\n",
        "\n",
        "    # Visualize clusters in 2D using PCA\n",
        "    pca = PCA(n_components=2)\n",
        "    pca_result = pca.fit_transform(scaled)\n",
        "    df_cluster['pca1'] = pca_result[:, 0]\n",
        "    df_cluster['pca2'] = pca_result[:, 1]\n",
        "\n",
        "    plt.figure(figsize=(10, 7))\n",
        "    sns.scatterplot(\n",
        "        data=df_cluster,\n",
        "        x='pca1', y='pca2',\n",
        "        hue='behavior_cluster',\n",
        "        palette='tab10',\n",
        "        size='size_usd',\n",
        "        sizes=(20, 200),\n",
        "        alpha=0.7,\n",
        "        legend='full'\n",
        "    )\n",
        "    plt.title(\"Trade Behavior Clusters (PCA 2D Projection)\")\n",
        "    plt.xlabel(\"PCA Component 1\")\n",
        "    plt.ylabel(\"PCA Component 2\")\n",
        "    plt.legend(title=\"Cluster\")\n",
        "    plt.grid(True, linestyle='--', alpha=0.5)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"outputs/trade_clusters_pca.png\")\n",
        "    plt.show()\n",
        "\n",
        "    print(\"→ Clustering complete! Check outputs/trade_clusters_pca.png and elbow_silhouette_kmeans.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ======================================================\n",
        "# Linear Regression – Standalone / Fixed Version\n",
        "# ======================================================\n",
        "print(\"\\n=== Starting Linear Regression ===\")\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ── Step 1: Load or use existing data ──\n",
        "try:\n",
        "    # If 'merged' already exists from main script → use it\n",
        "    daily_df = merged.copy()  # will raise NameError if merged not defined\n",
        "    print(\"Using existing 'merged' DataFrame from memory.\")\n",
        "except NameError:\n",
        "    # If merged not defined → load from saved CSV\n",
        "    print(\"Loading saved merged data from CSV...\")\n",
        "    file_path = \"csv_files/merged_trader_sentiment_processed.csv\"\n",
        "    if not os.path.exists(file_path):\n",
        "        raise FileNotFoundError(f\"Cannot find {file_path}. Run main ETL script first!\")\n",
        "    df = pd.read_csv(file_path, low_memory=False)\n",
        "    daily_df = df.copy()\n",
        "    print(\"Loaded from CSV.\")\n",
        "\n",
        "# ── Step 2: Daily aggregation for lagged analysis ──\n",
        "daily_df['trade_date'] = pd.to_datetime(daily_df['trade_date'])  # ensure date type\n",
        "\n",
        "daily_agg = daily_df.groupby('trade_date').agg({\n",
        "    'closed_pnl': 'mean',\n",
        "    'sent_value': 'mean',\n",
        "    'size_usd': 'mean'\n",
        "}).reset_index()\n",
        "\n",
        "# Create lagged features (previous day)\n",
        "daily_agg['lag_sent_value'] = daily_agg['sent_value'].shift(1)\n",
        "daily_agg['lag_size_usd']   = daily_agg['size_usd'].shift(1)\n",
        "\n",
        "# Drop rows with NaN (first day has no lag)\n",
        "daily_reg = daily_agg.dropna().copy()\n",
        "\n",
        "if daily_reg.empty:\n",
        "    print(\"ERROR: No valid rows left after creating lags and dropna. Check data range or missing values.\")\n",
        "else:\n",
        "    print(f\"Regression on {len(daily_reg)} valid daily records.\")\n",
        "\n",
        "    # ── Step 3: Prepare X and y ──\n",
        "    X = daily_reg[['lag_sent_value', 'lag_size_usd', 'sent_value']]  # predictors\n",
        "    y = daily_reg['closed_pnl']  # target: average daily PnL\n",
        "\n",
        "    # Time-series split (no random shuffle)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, shuffle=False\n",
        "    )\n",
        "\n",
        "    # ── Step 4: Fit model ──\n",
        "    reg = LinearRegression()\n",
        "    reg.fit(X_train, y_train)\n",
        "\n",
        "    # ── Step 5: Predict & Evaluate ──\n",
        "    y_pred = reg.predict(X_test)\n",
        "\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "\n",
        "    print(f\"R² Score (test): {r2:.3f}\")\n",
        "    print(f\"RMSE (test): {rmse:.4f}\")\n",
        "    print(\"\\nCoefficients:\")\n",
        "    for feat, coef in zip(X.columns, reg.coef_):\n",
        "        print(f\"  {feat:15} : {coef:.4f}\")\n",
        "    print(f\"Intercept: {reg.intercept_:.4f}\")\n",
        "\n",
        "    # ── Step 6: Plot Actual vs Predicted ──\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.scatter(y_test, y_pred, alpha=0.6, label='Predictions')\n",
        "    plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--', lw=2, label='Perfect fit')\n",
        "    plt.xlabel(\"Actual Average Daily PnL\")\n",
        "    plt.ylabel(\"Predicted Average Daily PnL\")\n",
        "    plt.title(\"Linear Regression: Actual vs Predicted\")\n",
        "    plt.legend()\n",
        "    plt.grid(True, linestyle='--', alpha=0.5)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"outputs/regression_actual_vs_pred.png\")\n",
        "    plt.show()\n",
        "\n",
        "    print(\"→ Regression complete! Check outputs/regression_actual_vs_pred.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "e9OAv3EdJD7v",
        "outputId": "52476430-49d8-457b-c38f-bb3e04a1faab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Created ds_submission.zip with all results\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_d0ebcd49-c88e-4550-9685-61d2fdf5d3f4\", \"ds_submission.zip\", 11624957)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# ============================================================\n",
        "# 9. Package all results into a zip for download\n",
        "# ============================================================\n",
        "from google.colab import files\n",
        "import zipfile\n",
        "\n",
        "out_zip = \"ds_submission.zip\"\n",
        "\n",
        "# Add everything into the zip\n",
        "with zipfile.ZipFile(out_zip, \"w\", zipfile.ZIP_DEFLATED) as zf:\n",
        "    # Report\n",
        "    if Path(\"ds_report.pdf\").exists():\n",
        "        zf.write(\"ds_report.pdf\")\n",
        "    # Folders\n",
        "    for folder in [\"outputs\", \"csv_files\"]:\n",
        "        if Path(folder).exists():\n",
        "            for file in Path(folder).rglob(\"*\"):\n",
        "                zf.write(file)\n",
        "\n",
        "print(f\"✅ Created {out_zip} with all results\")\n",
        "\n",
        "# Trigger download\n",
        "files.download(out_zip)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
